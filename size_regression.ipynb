{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files:   1%|▏         | 97/6813 [00:03<02:59, 37.38file/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x76ae22364050>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/aisystem/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Processing JSON files:   8%|▊         | 565/6813 [00:14<02:01, 51.27file/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_valid_furniture_data(json_dir):\n",
    "    result = []\n",
    "\n",
    "    bar = tqdm(os.listdir(json_dir), desc=\"Processing JSON files\", unit=\"file\")\n",
    "\n",
    "    for filename in bar:\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        furniture_list = data.get(\"furniture\", [])\n",
    "        for item in furniture_list:\n",
    "            if not item.get(\"valid\", False):\n",
    "                continue\n",
    "            if not all(k in item for k in (\"category\", \"size\", \"jid\")):\n",
    "                continue\n",
    "            result.append({\n",
    "                \"category\": item[\"category\"],\n",
    "                \"size\": item[\"size\"],\n",
    "                \"jid\": item[\"jid\"]\n",
    "            })\n",
    "\n",
    "    return result\n",
    "\n",
    "# 사용 예시\n",
    "json_directory = \"/home/eden/Documents/JNU/2025-1/AI-System/AI-System-Project/3D-FRONT\"\n",
    "furniture_data = extract_valid_furniture_data(json_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9947bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funiture_data 저장\n",
    "output_file = \"furniture_data.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(furniture_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385a28f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 5831개의 유효한 가구 데이터가 추출되었습니다.\n",
      "{'category': 'Lighting', 'size': [0.4941799926757813, 0.3422480010986328, 1.2236100006103516], 'jid': '71ab8f57-0cf2-4efb-83bd-6d87258622a6'}\n",
      "{'category': 'Cabinet/Shelf/Desk', 'size': [0.9, 0.6229999923706054, 0.5], 'jid': '02a64bc1-70d6-4da7-98c8-19038cfe95ec'}\n",
      "{'category': 'Cabinet/Shelf/Desk', 'size': [0.9, 0.625, 2.291999969482422], 'jid': 'a7cccc53-423b-401c-883d-60b4ed424507'}\n",
      "{'category': 'Lighting', 'size': [0.28629800796508786, 0.18640800476074218, 1.5544700622558594], 'jid': 'd214e531-662b-42c0-a180-846178536688'}\n",
      "{'category': 'Lighting', 'size': [0.5009120178222656, 0.4108599853515625, 0.05069169998168945], 'jid': 'b09d1f89-8d93-4263-82d4-264c49bd7ce5'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# funiture_data 로드\n",
    "output_file = \"furniture_data.json\"\n",
    "with open(output_file, 'r') as f:\n",
    "    furniture_data = json.load(f)\n",
    "\n",
    "print(f\"총 {len(furniture_data)}개의 유효한 가구 데이터가 추출되었습니다.\")\n",
    "\n",
    "# 결과 예시 출력\n",
    "for item in furniture_data[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e418889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lighting: 1511\n",
      "Cabinet/Shelf/Desk: 2020\n",
      "Table: 926\n",
      "Chair: 320\n",
      "Sofa: 379\n",
      "Bed: 439\n",
      "Pier/Stool: 141\n",
      "Others: 95\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# category 개수 세기\n",
    "category_counts = Counter(item[\"category\"] for item in furniture_data)\n",
    "\n",
    "# 출력\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f191474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 고유 카테고리 추출 및 인덱스 매핑\n",
    "category_set = sorted(set(item[\"category\"] for item in furniture_data))\n",
    "category_to_idx = {cat: i for i, cat in enumerate(category_set)}\n",
    "\n",
    "# 클래스 인덱스 리스트 생성\n",
    "class_indices = [category_to_idx[item[\"category\"]] for item in furniture_data]\n",
    "size_list = [item[\"size\"] for item in furniture_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42d610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max size: 5.52886\n",
      "min size: 0.00321001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "size_np = np.array(size_list, dtype=np.float32)\n",
    "print(\"max size:\", size_np.max())\n",
    "print(\"min size:\", size_np.min())\n",
    "\n",
    "normalized_size = (size_np - size_np.min()) / (size_np.max() - size_np.min())\n",
    "# 정규화된 사이즈를 리스트로 변환\n",
    "normalized_size_list = normalized_size.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2841e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "latent_vectors = []\n",
    "valid_class_indices = []\n",
    "valid_sizes = []\n",
    "\n",
    "def make_img_path(jid):\n",
    "    return os.path.join(\"/home/eden/Data/JNU/AI-System/3D-FUTURE-model\", f\"{jid}/image.jpg\")\n",
    "\n",
    "img_paths = [make_img_path(item[\"jid\"]) for item in furniture_data]\n",
    "batch_size = 4\n",
    "batch_img_paths = [img_paths[i:i + batch_size] for i in range(0, len(img_paths), batch_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cd528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded latent vectors of shape: torch.Size([1200, 1048576])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "saved_latent_vectors = torch.load(\"/home/eden/Data/JNU/AI-System/latent_vectors__.pt\")\n",
    "print(f\"Loaded latent vectors of shape: {saved_latent_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bf41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(len(saved_latent_vectors)):\n",
    "    latent_vectors.append(saved_latent_vectors[i])\n",
    "    valid_class_indices.append(class_indices[i])\n",
    "    valid_sizes.append(size_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5812fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FurnitureSizeRegressor(nn.Module):\n",
    "    def __init__(self, latent_dim, class_count, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.class_embed = nn.Embedding(class_count, 64)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim+64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            # nn.Sigmoid()  # Normalize output to [0, 1] range\n",
    "        )\n",
    "\n",
    "    def forward(self, latent_vec, class_idx):\n",
    "        class_vec = self.class_embed(class_idx)\n",
    "        x = torch.cat([latent_vec, class_vec], dim=-1)\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35611fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 960\n",
      "Test dataset size: 240\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class FurnitureDataset(Dataset):\n",
    "    def __init__(self, latent_vectors, class_indices, sizes):\n",
    "        self.latents = torch.from_numpy(np.array(latent_vectors)).float()\n",
    "        self.classes = torch.from_numpy(np.array(class_indices)).int()\n",
    "        self.sizes = torch.from_numpy(np.array(sizes)).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents[idx], self.classes[idx], self.sizes[idx]\n",
    "\n",
    "# 예시\n",
    "dataset = FurnitureDataset(latent_vectors, valid_class_indices, valid_sizes)\n",
    "\n",
    "# dataset을 test와 train으로 나눔\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "class FurnitureSubset(Dataset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "train_dataset = FurnitureSubset(dataset, train_indices)\n",
    "test_dataset = FurnitureSubset(dataset, test_indices)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c7dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: 1048576, Class count: 8\n"
     ]
    }
   ],
   "source": [
    "latent_dim = latent_vectors[0].shape[0]  # latent vector의 차원\n",
    "class_count = len(category_set)\n",
    "\n",
    "print(f\"Latent dimension: {latent_dim}, Class count: {class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f593be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 30/30 [00:02<00:00, 11.28batch/s, loss=40]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1748.2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 35.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 43.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 30/30 [00:02<00:00, 11.78batch/s, loss=8.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 18.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.5804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 30/30 [00:02<00:00, 11.76batch/s, loss=5.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 4.7823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 30/30 [00:02<00:00, 11.71batch/s, loss=1.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 2.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 30/30 [00:02<00:00, 11.77batch/s, loss=0.878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.0626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 30/30 [00:02<00:00, 11.72batch/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 30/30 [00:02<00:00, 11.80batch/s, loss=0.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 30/30 [00:02<00:00, 11.81batch/s, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.4324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 30/30 [00:02<00:00, 11.77batch/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 30/30 [00:02<00:00, 11.76batch/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.2759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 8/8 [00:00<00:00, 34.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6844\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_dataloader, test_dataloader, epochs=10, lr=1e-3):\n",
    "    device = next(model.parameters()).device\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_bar = tqdm(train_dataloader, total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "        for latents, classes, sizes in train_bar:\n",
    "            latents, classes, sizes = latents.to(device), classes.to(device), sizes.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(latents, classes)\n",
    "            loss = criterion(outputs, sizes)\n",
    "            train_bar.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_bar = tqdm(test_dataloader, total=len(test_dataloader), desc=\"Testing\", unit=\"batch\")\n",
    "        with torch.no_grad():\n",
    "            total_test_loss = 0.0\n",
    "            for latents, classes, sizes in test_bar:\n",
    "                latents, classes, sizes = latents.to(device), classes.to(device), sizes.to(device)\n",
    "                outputs = model(latents, classes)\n",
    "                loss = criterion(outputs, sizes)\n",
    "                total_test_loss += loss.item()\n",
    "            avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "            print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 모델 초기화 및 학습\n",
    "latent_dim = latent_vectors[0].shape[0]  # latent vector의 차원\n",
    "class_count = len(category_set)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FurnitureSizeRegressor(latent_dim, class_count).to(device)\n",
    "train_model(model, train_dataloader, test_dataloader, epochs=10, lr=1e-3)\n",
    "\n",
    "# 모델 저장\n",
    "output_model_path = \"furniture_size_regressor.pth\"\n",
    "torch.save(model.state_dict(), output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47c1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
